{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LmY3G9stvUjz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634299782554,"user_tz":300,"elapsed":34276,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}},"outputId":"090426a5-b4ec-459f-9007-abc564eae9ad"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"D4chz_U5v1kA","executionInfo":{"status":"ok","timestamp":1634299789802,"user_tz":300,"elapsed":999,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}}},"source":["from pandas import read_csv\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkG8ZC0xv5g2","executionInfo":{"status":"ok","timestamp":1634299792273,"user_tz":300,"elapsed":332,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}}},"source":["##Load the dataset\n","#df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/COSC4314/diabetes.csv')\n","\n","#Print the first 5 rows of the dataframe.\n","#df.head()\n","\n","url = \"https://raw.githubusercontent.com/rrichajalota/Pima-Indians-Diabetes-kaggle/master/diabetes.csv\"\n","path = '/content/drive/My Drive/Colab Notebooks/COSC4314/'\n","filename = 'diabetes.csv'\n","\n","#dataframe = read_csv(path+filename)\n","dataframe = read_csv(url)\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"YKN3lGjzv839","executionInfo":{"status":"ok","timestamp":1634299806977,"user_tz":300,"elapsed":87,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}}},"source":["array = dataframe.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_r-NLwswBms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634299818103,"user_tz":300,"elapsed":90,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}},"outputId":"9433ab03-8be2-4709-afb2-2df02454675e"},"source":["kfold = KFold(n_splits=5, random_state=2, shuffle=True)\n","model = KNeighborsClassifier(n_neighbors=3)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7161701044053984\n"]}]},{"cell_type":"code","metadata":{"id":"RMbKRFsC5lyM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634299845193,"user_tz":300,"elapsed":126,"user":{"displayName":"Hyuk Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10374101826971021363"}},"outputId":"150fdd22-c194-4bbc-fb23-aae2aa45e48a"},"source":["help(KNeighborsClassifier\n","     )"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class KNeighborsClassifier in module sklearn.neighbors._classification:\n","\n","class KNeighborsClassifier(sklearn.neighbors._base.NeighborsBase, sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.SupervisedIntegerMixin, sklearn.base.ClassifierMixin)\n"," |  KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n"," |  \n"," |  Classifier implementing the k-nearest neighbors vote.\n"," |  \n"," |  Read more in the :ref:`User Guide <classification>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_neighbors : int, optional (default = 5)\n"," |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n"," |  \n"," |  weights : str or callable, optional (default = 'uniform')\n"," |      weight function used in prediction.  Possible values:\n"," |  \n"," |      - 'uniform' : uniform weights.  All points in each neighborhood\n"," |        are weighted equally.\n"," |      - 'distance' : weight points by the inverse of their distance.\n"," |        in this case, closer neighbors of a query point will have a\n"," |        greater influence than neighbors which are further away.\n"," |      - [callable] : a user-defined function which accepts an\n"," |        array of distances, and returns an array of the same shape\n"," |        containing the weights.\n"," |  \n"," |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n"," |      Algorithm used to compute the nearest neighbors:\n"," |  \n"," |      - 'ball_tree' will use :class:`BallTree`\n"," |      - 'kd_tree' will use :class:`KDTree`\n"," |      - 'brute' will use a brute-force search.\n"," |      - 'auto' will attempt to decide the most appropriate algorithm\n"," |        based on the values passed to :meth:`fit` method.\n"," |  \n"," |      Note: fitting on sparse input will override the setting of\n"," |      this parameter, using brute force.\n"," |  \n"," |  leaf_size : int, optional (default = 30)\n"," |      Leaf size passed to BallTree or KDTree.  This can affect the\n"," |      speed of the construction and query, as well as the memory\n"," |      required to store the tree.  The optimal value depends on the\n"," |      nature of the problem.\n"," |  \n"," |  p : integer, optional (default = 2)\n"," |      Power parameter for the Minkowski metric. When p = 1, this is\n"," |      equivalent to using manhattan_distance (l1), and euclidean_distance\n"," |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n"," |  \n"," |  metric : string or callable, default 'minkowski'\n"," |      the distance metric to use for the tree.  The default metric is\n"," |      minkowski, and with p=2 is equivalent to the standard Euclidean\n"," |      metric. See the documentation of the DistanceMetric class for a\n"," |      list of available metrics.\n"," |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n"," |      must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n"," |      in which case only \"nonzero\" elements may be considered neighbors.\n"," |  \n"," |  metric_params : dict, optional (default = None)\n"," |      Additional keyword arguments for the metric function.\n"," |  \n"," |  n_jobs : int or None, optional (default=None)\n"," |      The number of parallel jobs to run for neighbors search.\n"," |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n"," |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n"," |      for more details.\n"," |      Doesn't affect :meth:`fit` method.\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  classes_ : array of shape (n_classes,)\n"," |      Class labels known to the classifier\n"," |  \n"," |  effective_metric_ : string or callble\n"," |      The distance metric used. It will be same as the `metric` parameter\n"," |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n"," |      'minkowski' and `p` parameter set to 2.\n"," |  \n"," |  effective_metric_params_ : dict\n"," |      Additional keyword arguments for the metric function. For most metrics\n"," |      will be same with `metric_params` parameter, but may also contain the\n"," |      `p` parameter value if the `effective_metric_` attribute is set to\n"," |      'minkowski'.\n"," |  \n"," |  outputs_2d_ : bool\n"," |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n"," |      otherwise True.\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> X = [[0], [1], [2], [3]]\n"," |  >>> y = [0, 0, 1, 1]\n"," |  >>> from sklearn.neighbors import KNeighborsClassifier\n"," |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n"," |  >>> neigh.fit(X, y)\n"," |  KNeighborsClassifier(...)\n"," |  >>> print(neigh.predict([[1.1]]))\n"," |  [0]\n"," |  >>> print(neigh.predict_proba([[0.9]]))\n"," |  [[0.66666667 0.33333333]]\n"," |  \n"," |  See also\n"," |  --------\n"," |  RadiusNeighborsClassifier\n"," |  KNeighborsRegressor\n"," |  RadiusNeighborsRegressor\n"," |  NearestNeighbors\n"," |  \n"," |  Notes\n"," |  -----\n"," |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n"," |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n"," |  \n"," |  .. warning::\n"," |  \n"," |     Regarding the Nearest Neighbors algorithms, if it is found that two\n"," |     neighbors, neighbor `k+1` and `k`, have identical distances\n"," |     but different labels, the results will depend on the ordering of the\n"," |     training data.\n"," |  \n"," |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"," |  \n"," |  Method resolution order:\n"," |      KNeighborsClassifier\n"," |      sklearn.neighbors._base.NeighborsBase\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.neighbors._base.KNeighborsMixin\n"," |      sklearn.neighbors._base.SupervisedIntegerMixin\n"," |      sklearn.base.ClassifierMixin\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  predict(self, X)\n"," |      Predict the class labels for the provided data.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          Test samples.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : array of shape [n_queries] or [n_queries, n_outputs]\n"," |          Class labels for each data sample.\n"," |  \n"," |  predict_proba(self, X)\n"," |      Return probability estimates for the test data X.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          Test samples.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : array of shape = [n_queries, n_classes], or a list of n_outputs\n"," |          of such arrays if n_outputs > 1.\n"," |          The class probabilities of the input samples. Classes are ordered\n"," |          by lexicographic order.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : mapping of string to any\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as pipelines). The latter have parameters of the form\n"," |      ``<component>__<parameter>`` so that it's possible to update each\n"," |      component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          Estimator instance.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n"," |  \n"," |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n"," |      Finds the K-neighbors of a point.\n"," |      Returns indices of and distances to the neighbors of each point.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          The query point or points.\n"," |          If not provided, neighbors of each indexed point are returned.\n"," |          In this case, the query point is not considered its own neighbor.\n"," |      \n"," |      n_neighbors : int\n"," |          Number of neighbors to get (default is the value\n"," |          passed to the constructor).\n"," |      \n"," |      return_distance : boolean, optional. Defaults to True.\n"," |          If False, distances will not be returned\n"," |      \n"," |      Returns\n"," |      -------\n"," |      neigh_dist : array, shape (n_queries, n_neighbors)\n"," |          Array representing the lengths to points, only present if\n"," |          return_distance=True\n"," |      \n"," |      neigh_ind : array, shape (n_queries, n_neighbors)\n"," |          Indices of the nearest points in the population matrix.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      In the following example, we construct a NearestNeighbors\n"," |      class from an array representing our data set and ask who's\n"," |      the closest point to [1,1,1]\n"," |      \n"," |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n"," |      >>> from sklearn.neighbors import NearestNeighbors\n"," |      >>> neigh = NearestNeighbors(n_neighbors=1)\n"," |      >>> neigh.fit(samples)\n"," |      NearestNeighbors(n_neighbors=1)\n"," |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n"," |      (array([[0.5]]), array([[2]]))\n"," |      \n"," |      As you can see, it returns [[0.5]], and [[2]], which means that the\n"," |      element is at distance 0.5 and is the third element of samples\n"," |      (indexes start at 0). You can also query for multiple points:\n"," |      \n"," |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n"," |      >>> neigh.kneighbors(X, return_distance=False)\n"," |      array([[1],\n"," |             [2]]...)\n"," |  \n"," |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n"," |      Computes the (weighted) graph of k-Neighbors for points in X\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          The query point or points.\n"," |          If not provided, neighbors of each indexed point are returned.\n"," |          In this case, the query point is not considered its own neighbor.\n"," |      \n"," |      n_neighbors : int\n"," |          Number of neighbors for each sample.\n"," |          (default is value passed to the constructor).\n"," |      \n"," |      mode : {'connectivity', 'distance'}, optional\n"," |          Type of returned matrix: 'connectivity' will return the\n"," |          connectivity matrix with ones and zeros, in 'distance' the\n"," |          edges are Euclidean distance between points.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      A : sparse graph in CSR format, shape = [n_queries, n_samples_fit]\n"," |          n_samples_fit is the number of samples in the fitted data\n"," |          A[i, j] is assigned the weight of edge that connects i to j.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> X = [[0], [3], [1]]\n"," |      >>> from sklearn.neighbors import NearestNeighbors\n"," |      >>> neigh = NearestNeighbors(n_neighbors=2)\n"," |      >>> neigh.fit(X)\n"," |      NearestNeighbors(n_neighbors=2)\n"," |      >>> A = neigh.kneighbors_graph(X)\n"," |      >>> A.toarray()\n"," |      array([[1., 0., 1.],\n"," |             [0., 1., 1.],\n"," |             [1., 0., 1.]])\n"," |      \n"," |      See also\n"," |      --------\n"," |      NearestNeighbors.radius_neighbors_graph\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.neighbors._base.SupervisedIntegerMixin:\n"," |  \n"," |  fit(self, X, y)\n"," |      Fit the model using X as training data and y as target values\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix, BallTree, KDTree}\n"," |          Training data. If array or matrix, shape [n_samples, n_features],\n"," |          or [n_samples, n_samples] if metric='precomputed'.\n"," |      \n"," |      y : {array-like, sparse matrix}\n"," |          Target values of shape = [n_samples] or [n_samples, n_outputs]\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for X.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of self.predict(X) wrt. y.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"F1Htt6GN3Jk2"},"source":[""],"execution_count":null,"outputs":[]}]}